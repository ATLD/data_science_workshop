{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full NLP and ML Pipeline for Document Classification\n",
    "Based on following tutorials   \n",
    "With Permission from Michale Harmon:\n",
    "http://michael-harmon.com/blog/NLP.html   \n",
    "https://github.com/mdh266/DocumentClassificationNLP/blob/master/NLP.ipynb   \n",
    "\n",
    "Classification of text documents using sparse features:   \n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html   \n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  20 News Groups Corpus, Sample dataset included in scikit-learn\n",
    "A collection of almost 20,000 articles on 20 different topics or 'newsgroups'.   \n",
    "Corpus: Text Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 classes (of 20)\n",
    "twenty_train.target_names[0:3]\n",
    "# python indexing excludes end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "# data and target\n",
    "# has the input and desired output\n",
    "# 11K of them are split for training pairs\n",
    "\n",
    "print( len(twenty_train.data) )\n",
    "print( len(twenty_train.target) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jimf@centerline.com (Jim Frost)\n",
      "Subject: Re: Is car saftey important?\n",
      "Organization: CenterLine Software, Inc.\n",
      "Lines: 14\n",
      "NNTP-Posting-Host: 140.239.3.202\n",
      "\n",
      "tcorkum@bnr.ca (Trevor Corkum) writes:\n",
      ">Is it only me, or is\n",
      ">safety not one of the most important factors when buying a car?\n",
      "\n",
      "It depends on your priorities.  A lot of people put higher priorities\n",
      "on gas mileage and cost than on safety, buying \"unsafe\" econoboxes\n",
      "instead of Volvos.  I personally take a middle ground -- the only\n",
      "thing I really look for is a three-point seatbelt and 5+mph bumpers.\n",
      "I figure that 30mph collisions into brick walls aren't common enough\n",
      "for me to spend that much extra money for protection, but there are\n",
      "lots of low-speed collisions that do worry me.\n",
      "\n",
      "jim frost\n",
      "jimf@centerline.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 29\n",
    "print(twenty_train.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.autos'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[  twenty_train.target[i]  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn Pipeline\n",
    "- Scitkit-learn pipelines are a sequence of transforms followed by a final estimator.   \n",
    "- Intermediate steps within the pipeline must be ‘transforms’ \n",
    " * they must implement fit and transform methods \n",
    " * The CountVectorizer and TfidfTransformer are transformers in this example   \n",
    "- The estimator of a pipeline, the final step, only needs to implement the fit method   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.81      0.80       319\n",
      "           comp.graphics       0.65      0.80      0.72       389\n",
      " comp.os.ms-windows.misc       0.80      0.04      0.08       394\n",
      "comp.sys.ibm.pc.hardware       0.55      0.80      0.65       392\n",
      "   comp.sys.mac.hardware       0.85      0.79      0.82       385\n",
      "          comp.windows.x       0.69      0.84      0.76       395\n",
      "            misc.forsale       0.89      0.74      0.81       390\n",
      "               rec.autos       0.89      0.92      0.91       396\n",
      "         rec.motorcycles       0.95      0.94      0.95       398\n",
      "      rec.sport.baseball       0.95      0.92      0.93       397\n",
      "        rec.sport.hockey       0.92      0.97      0.94       399\n",
      "               sci.crypt       0.80      0.96      0.87       396\n",
      "         sci.electronics       0.79      0.70      0.74       393\n",
      "                 sci.med       0.88      0.87      0.87       396\n",
      "               sci.space       0.84      0.92      0.88       394\n",
      "  soc.religion.christian       0.81      0.95      0.87       398\n",
      "      talk.politics.guns       0.72      0.93      0.81       364\n",
      "   talk.politics.mideast       0.93      0.94      0.94       376\n",
      "      talk.politics.misc       0.68      0.62      0.65       310\n",
      "      talk.religion.misc       0.88      0.44      0.59       251\n",
      "\n",
      "                accuracy                           0.80      7532\n",
      "               macro avg       0.81      0.79      0.78      7532\n",
      "            weighted avg       0.81      0.80      0.78      7532\n",
      "\n",
      "Accuracy: 0.8023101433882103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                 ('model', MultinomialNB()),])\n",
    "\n",
    "mod = pipe.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = mod.predict(twenty_test.data)\n",
    "\n",
    "print(classification_report(twenty_test.target,\n",
    "                            predicted, \n",
    "                            target_names=twenty_test.target_names))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.69      0.74       319\n",
      "           comp.graphics       0.78      0.72      0.75       389\n",
      " comp.os.ms-windows.misc       0.79      0.72      0.75       394\n",
      "comp.sys.ibm.pc.hardware       0.68      0.81      0.74       392\n",
      "   comp.sys.mac.hardware       0.86      0.81      0.84       385\n",
      "          comp.windows.x       0.87      0.78      0.82       395\n",
      "            misc.forsale       0.87      0.80      0.83       390\n",
      "               rec.autos       0.88      0.91      0.90       396\n",
      "         rec.motorcycles       0.93      0.96      0.95       398\n",
      "      rec.sport.baseball       0.91      0.92      0.92       397\n",
      "        rec.sport.hockey       0.88      0.98      0.93       399\n",
      "               sci.crypt       0.75      0.96      0.84       396\n",
      "         sci.electronics       0.84      0.65      0.74       393\n",
      "                 sci.med       0.92      0.79      0.85       396\n",
      "               sci.space       0.82      0.94      0.88       394\n",
      "  soc.religion.christian       0.62      0.96      0.76       398\n",
      "      talk.politics.guns       0.66      0.95      0.78       364\n",
      "   talk.politics.mideast       0.95      0.94      0.94       376\n",
      "      talk.politics.misc       0.94      0.52      0.67       310\n",
      "      talk.religion.misc       0.95      0.24      0.38       251\n",
      "\n",
      "                accuracy                           0.82      7532\n",
      "               macro avg       0.84      0.80      0.80      7532\n",
      "            weighted avg       0.83      0.82      0.81      7532\n",
      "\n",
      "Accuracy: 0.8169144981412639\n"
     ]
    }
   ],
   "source": [
    "# Easyly to add, remove, or modify steps and retest \n",
    "pipe = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                  ('tfidf', TfidfTransformer()), # Added TFIDF\n",
    "                  ('model', MultinomialNB()),])\n",
    "\n",
    "mod = pipe.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted = mod.predict(twenty_test.data)\n",
    "\n",
    "print(classification_report(twenty_test.target,\n",
    "                            predicted, \n",
    "                            target_names=twenty_test.target_names))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRY: remove stop_words removal and see change in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting, and Hyperparameter tuning using GridSearchCV and Pipeline\n",
    "Similar to testing whether to remove stopwords or not, we want to run many experiments, with many combinations of parameters.  \n",
    "GridSearchCV does this, for an estimator, or on a full pipeline. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html   \n",
    "   \n",
    "For  setting parameters of the various steps in the \n",
    "pipeline, you use <step name>+\"__\"+<parameter name>, for the list of possible values you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiments\n",
    "    \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__stop_words': ('english', None)}\n",
    "\n",
    "# We can perform the grid search using all available CPU by setting n_jobs=-1:\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip_accents=Non...\n",
       "                                                        tokenizer=None,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('tfidf',\n",
       "                                        TfidfTransformer(norm='l2',\n",
       "                                                         smooth_idf=True,\n",
       "                                                         sublinear_tf=False,\n",
       "                                                         use_idf=True)),\n",
       "                                       ('model',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'vect__stop_words': ('english', None)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the experiments \n",
    "grid_search.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.06957569, 4.12887521]),\n",
       " 'std_fit_time': array([0.05679224, 0.12050998]),\n",
       " 'mean_score_time': array([0.78699408, 0.56292038]),\n",
       " 'std_score_time': array([0.09046821, 0.04964514]),\n",
       " 'param_vect__stop_words': masked_array(data=['english', None],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'vect__stop_words': 'english'}, {'vect__stop_words': None}],\n",
       " 'split0_test_score': array([0.88497135, 0.84883208]),\n",
       " 'split1_test_score': array([0.88393645, 0.84245366]),\n",
       " 'split2_test_score': array([0.88030035, 0.84452297]),\n",
       " 'split3_test_score': array([0.87699115, 0.84159292]),\n",
       " 'split4_test_score': array([0.88292683, 0.84345898]),\n",
       " 'mean_test_score': array([0.88182782, 0.84417536]),\n",
       " 'std_test_score': array([0.00287277, 0.00253006]),\n",
       " 'rank_test_score': array([1, 2], dtype=int32)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of \n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As GridSearchCV already does cross validation, we could combine train and test data and feed all to GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('model',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip_accents=Non...\n",
       "                                        TfidfTransformer(norm='l2',\n",
       "                                                         smooth_idf=True,\n",
       "                                                         sublinear_tf=False,\n",
       "                                                         use_idf=True)),\n",
       "                                       ('model',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'model__alpha': (10.0, 0.001),\n",
       "                         'model__fit_prior': (True, False),\n",
       "                         'tfidf__use_idf': (True, False)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters2 = {'tfidf__use_idf': (True, False),\n",
    "              'model__alpha': (1e1, 1e-3),\n",
    "              'model__fit_prior': (True,False)}\n",
    "\n",
    "\n",
    "grid_search2 = GridSearchCV(pipe, parameters2, n_jobs=-1, cv=5)\n",
    "grid_search2.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([4.29649329, 4.8832726 , 5.18208666, 5.3344738 , 5.06866918,\n",
       "        5.33335633, 5.27376876, 3.95952954]),\n",
       " 'std_fit_time': array([0.33189017, 0.1463874 , 0.12465923, 0.10098456, 0.04063029,\n",
       "        0.03239787, 0.13931573, 0.6709754 ]),\n",
       " 'mean_score_time': array([1.1397922 , 1.16010547, 1.16209259, 1.12569327, 1.0870935 ,\n",
       "        1.13249288, 0.9435956 , 0.55339794]),\n",
       " 'std_score_time': array([0.04132594, 0.12882778, 0.11581208, 0.03641871, 0.03628358,\n",
       "        0.06997059, 0.12525417, 0.06948461]),\n",
       " 'param_model__alpha': masked_array(data=[10.0, 10.0, 10.0, 10.0, 0.001, 0.001, 0.001, 0.001],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_model__fit_prior': masked_array(data=[True, True, False, False, True, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, False, True, False, True, False, True, False],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__alpha': 10.0,\n",
       "   'model__fit_prior': True,\n",
       "   'tfidf__use_idf': True},\n",
       "  {'model__alpha': 10.0, 'model__fit_prior': True, 'tfidf__use_idf': False},\n",
       "  {'model__alpha': 10.0, 'model__fit_prior': False, 'tfidf__use_idf': True},\n",
       "  {'model__alpha': 10.0, 'model__fit_prior': False, 'tfidf__use_idf': False},\n",
       "  {'model__alpha': 0.001, 'model__fit_prior': True, 'tfidf__use_idf': True},\n",
       "  {'model__alpha': 0.001, 'model__fit_prior': True, 'tfidf__use_idf': False},\n",
       "  {'model__alpha': 0.001, 'model__fit_prior': False, 'tfidf__use_idf': True},\n",
       "  {'model__alpha': 0.001, 'model__fit_prior': False, 'tfidf__use_idf': False}],\n",
       " 'split0_test_score': array([0.81533715, 0.75363596, 0.8453063 , 0.78007933, 0.910974  ,\n",
       "        0.91141472, 0.91009255, 0.91185544]),\n",
       " 'split1_test_score': array([0.81729921, 0.76434245, 0.85260371, 0.78067079, 0.90600177,\n",
       "        0.91041483, 0.90644307, 0.91173875]),\n",
       " 'split2_test_score': array([0.81537102, 0.7504417 , 0.85335689, 0.78047703, 0.90106007,\n",
       "        0.90768551, 0.90061837, 0.90812721]),\n",
       " 'split3_test_score': array([0.82876106, 0.7579646 , 0.86327434, 0.78053097, 0.9119469 ,\n",
       "        0.9159292 , 0.91150442, 0.91548673]),\n",
       " 'split4_test_score': array([0.82660754, 0.75964523, 0.85898004, 0.78492239, 0.90776053,\n",
       "        0.91396896, 0.90776053, 0.91529933]),\n",
       " 'mean_test_score': array([0.82066466, 0.75720346, 0.8546933 , 0.78133286, 0.90754817,\n",
       "        0.91187909, 0.90728301, 0.91249779]),\n",
       " 'std_test_score': array([0.00580528, 0.00481799, 0.00610529, 0.00180167, 0.00388975,\n",
       "        0.00285287, 0.00377063, 0.00271467]),\n",
       " 'rank_test_score': array([6, 8, 5, 7, 3, 2, 4, 1], dtype=int32)}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=False)),\n",
       "                ('model',\n",
       "                 MultinomialNB(alpha=0.001, class_prior=None,\n",
       "                               fit_prior=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.85      0.81      0.83       319\n",
      "           comp.graphics       0.66      0.74      0.70       389\n",
      " comp.os.ms-windows.misc       0.72      0.63      0.67       394\n",
      "comp.sys.ibm.pc.hardware       0.65      0.72      0.68       392\n",
      "   comp.sys.mac.hardware       0.83      0.82      0.83       385\n",
      "          comp.windows.x       0.83      0.76      0.80       395\n",
      "            misc.forsale       0.80      0.82      0.81       390\n",
      "               rec.autos       0.89      0.89      0.89       396\n",
      "         rec.motorcycles       0.94      0.96      0.95       398\n",
      "      rec.sport.baseball       0.96      0.93      0.94       397\n",
      "        rec.sport.hockey       0.94      0.97      0.96       399\n",
      "               sci.crypt       0.89      0.94      0.91       396\n",
      "         sci.electronics       0.80      0.74      0.77       393\n",
      "                 sci.med       0.90      0.83      0.86       396\n",
      "               sci.space       0.87      0.91      0.89       394\n",
      "  soc.religion.christian       0.87      0.93      0.90       398\n",
      "      talk.politics.guns       0.78      0.89      0.83       364\n",
      "   talk.politics.mideast       0.97      0.93      0.95       376\n",
      "      talk.politics.misc       0.76      0.66      0.70       310\n",
      "      talk.religion.misc       0.70      0.67      0.68       251\n",
      "\n",
      "                accuracy                           0.83      7532\n",
      "               macro avg       0.83      0.83      0.83      7532\n",
      "            weighted avg       0.83      0.83      0.83      7532\n",
      "\n",
      "Accuracy: 0.8325809877854488\n"
     ]
    }
   ],
   "source": [
    "predicted2 = grid_search2.predict(twenty_test.data)\n",
    "\n",
    "print(classification_report(twenty_test.target,\n",
    "                            predicted2, \n",
    "                            target_names=twenty_test.target_names))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(twenty_test.target, predicted2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increased from 0.816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016000000000000014"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.832 - 0.816"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 or 2 more documents classified correctly as a result of hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "\n",
    "- Save the model to file\n",
    "- Implement a web service that will load the model\n",
    " * Make prediction when a request is received using model\n",
    " * return the prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
