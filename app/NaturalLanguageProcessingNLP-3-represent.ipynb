{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text using Numerical Vectors    \n",
    "Feature Vectors from Text\n",
    "\n",
    "References:   \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction   \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding\n",
    "One Hot encoding is used in converting categorical variables into features or columns and coding one or zero for the presence of that particular category. When applied to documents, the number of features is going to be the number of total tokens present in the whole corpus.   \n",
    "\n",
    "This example is representing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>NLP</th>\n",
       "      <th>am</th>\n",
       "      <th>learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  NLP  am  learning\n",
       "0  1    0   0         0\n",
       "1  0    0   1         0\n",
       "2  0    0   0         1\n",
       "3  0    1   0         0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Text = \"I am learning NLP\"\n",
    "\n",
    "pd.get_dummies(Text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "Similar to One Hot, but instead of just storing \"1\" for exists, we keep the frequency (number of appearances) of the word (groups) in the document, or sentence.\n",
    "\n",
    "More:   \n",
    "https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'document', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'quick', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "# import pandas and sklearn's CountVectorizer class\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# set of documents\n",
    "corpus = ['The quick brown fox jumps over the lazy dog!',\n",
    "           'This document is the second document.']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "vectorizer.fit(corpus)\n",
    "# convert the documents into a document-term matrix\n",
    "X = vectorizer.transform(corpus)\n",
    "\n",
    "# or together: fit_transform\n",
    "# X = vectorizer.fit_transform(corpora)\n",
    "\n",
    "# retrieve the terms found in the corpora\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:   \n",
    "Why do you think output is represented in Compressed Sparse Row format? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0 1 1 1 1 0 2 0]\n",
      " [0 2 0 0 1 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Each row represents a document.\n",
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['brown', 'dog', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the'],\n",
       "       dtype='<U8'),\n",
       " array(['document', 'is', 'second', 'the', 'this'], dtype='<U8')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>document</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>is</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown  document  dog  fox  is  jumps  lazy  over  quick  second  the  \\\n",
       "Doc1      1         0    1    1   0      1     1     1      1       0    2   \n",
       "Doc2      0         2    0    0   1      0     0     0      0       1    1   \n",
       "\n",
       "      this  \n",
       "Doc1     0  \n",
       "Doc2     1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from a word matrix\n",
    "pd.DataFrame(data=X.toarray(), index=['Doc1', 'Doc2'],\n",
    "             columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>document</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>is</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown  document  dog  fox  is  jumps  lazy  over  quick  second  the  \\\n",
       "Doc0      1         0    1    1   0      1     1     1      1       0    2   \n",
       "Doc1      0         2    0    0   1      0     0     0      0       1    1   \n",
       "\n",
       "      this  \n",
       "Doc0     0  \n",
       "Doc1     1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To display for learning.\n",
    "# don't actually convert sparse matrix to dataframe in production\n",
    "\n",
    "# We can write a function to create the dataframe from the word matrix\n",
    "def word_matrix2df(word_matrix, feat_names):\n",
    "    \n",
    "    # create an index for each row\n",
    "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(word_matrix)]\n",
    "    df = pd.DataFrame(data=word_matrix.toarray(), index=doc_names,\n",
    "                      columns=feat_names)\n",
    "    return(df)\n",
    "\n",
    "# create a dataframe from the matrix\n",
    "word_matrix2df(X, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:    \n",
    "Now you define a 5th document and encode it in (represent it with) the vocabulary for above documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus2 = ['This is the fifth document.']\n",
    "# fifth is not in vocabulary\n",
    "X2 = vectorizer.transform(corpus2)\n",
    "print(X2.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['document', 'is', 'the', 'this'], dtype='<U8')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word \"fifth\" is lost as you see when we revert back to words\n",
    "vectorizer.inverse_transform(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNet lemmatizer    \n",
    "WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
    "\n",
    "WordNet superficially resembles a thesaurus. It groups words together based on their meanings.   \n",
    "2 important distinctions:   \n",
    "1) WordNet interlinks not just word forms but specific senses of words.   \n",
    "2) WordNet labels the semantic relations among words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ozgozt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(\"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We have use POS Tagging before word lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you give pos (\"part of speech\") tag, you get better lemmas (root words)\n",
    "# pos defaults to noun \n",
    "wordnet_lemmatizer.lemmatize(\"are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence.\n",
    "\n",
    "N-grams to the rescue! Instead of building a simple collection of unigrams (n=1) (bag of words), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted.   \n",
    "\n",
    "Bigram is the combination of 2 words.\n",
    "Trigram is 3 words and so on.\n",
    "\n",
    "For “I am learning NLP”, here are first 3 n-gram sets:   \n",
    "Unigrams: “I”, “am”, “ learning”, “NLP”   \n",
    "Bigrams: “I am”, “am learning”, “learning NLP”   \n",
    "Trigrams: “I am learning”, “am learning NLP”   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ozgozt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'am']),\n",
       " WordList(['am', 'learning']),\n",
       " WordList(['learning', 'NLP'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TextBlob depends on the Punkt Tokenizer Models (13 MB download zipped)\n",
    "nltk.download('punkt')\n",
    "\n",
    "Text = \"I am learning NLP\"\n",
    "# we can use textblob library:\n",
    "from textblob import TextBlob\n",
    "TextBlob(Text).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer with n-grams\n",
    "We can directly ask CountVectorizer to generate frequencies of n-grams, instead of unigrams only...    \n",
    "Same code as above, except \"ngram_range=(1,3)\" parameter passed to CountVectorizer class constructor.   \n",
    "Previous model had a vector of length 12.    \n",
    "#### Q:   \n",
    "What is the size of current vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brown', 'brown fox', 'brown fox jumps', 'document', 'document is', 'document is the', 'dog', 'fox', 'fox jumps', 'fox jumps over', 'is', 'is the', 'is the second', 'jumps', 'jumps over', 'jumps over the', 'lazy', 'lazy dog', 'over', 'over the', 'over the lazy', 'quick', 'quick brown', 'quick brown fox', 'second', 'second document', 'the', 'the lazy', 'the lazy dog', 'the quick', 'the quick brown', 'the second', 'the second document', 'this', 'this document', 'this document is']\n"
     ]
    }
   ],
   "source": [
    "# import pandas and sklearn's CountVectorizer class\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# set of documents\n",
    "corpus = ['The quick brown fox jumps over the lazy dog!',\n",
    "           'This document is the second document.']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "vectorizer.fit(corpus)\n",
    "# convert the documents into a document-term matrix\n",
    "X = vectorizer.transform(corpus)\n",
    "\n",
    "# or together: fit_transform\n",
    "# X = vectorizer.fit_transform(corpora)\n",
    "\n",
    "# retrieve the terms found in the corpora\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>brown fox jumps</th>\n",
       "      <th>document</th>\n",
       "      <th>document is</th>\n",
       "      <th>document is the</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>fox jumps over</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>the lazy</th>\n",
       "      <th>the lazy dog</th>\n",
       "      <th>the quick</th>\n",
       "      <th>the quick brown</th>\n",
       "      <th>the second</th>\n",
       "      <th>the second document</th>\n",
       "      <th>this</th>\n",
       "      <th>this document</th>\n",
       "      <th>this document is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      brown  brown fox  brown fox jumps  document  document is  \\\n",
       "Doc0      1          1                1         0            0   \n",
       "Doc1      0          0                0         2            1   \n",
       "\n",
       "      document is the  dog  fox  fox jumps  fox jumps over  ...  the  \\\n",
       "Doc0                0    1    1          1               1  ...    2   \n",
       "Doc1                1    0    0          0               0  ...    1   \n",
       "\n",
       "      the lazy  the lazy dog  the quick  the quick brown  the second  \\\n",
       "Doc0         1             1          1                1           0   \n",
       "Doc1         0             0          0                0           1   \n",
       "\n",
       "      the second document  this  this document  this document is  \n",
       "Doc0                    0     0              0                 0  \n",
       "Doc1                    1     1              1                 1  \n",
       "\n",
       "[2 rows x 36 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to display for learning.\n",
    "# don't actually convert sparse matrix to dataframe in production\n",
    "word_matrix2df(X, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Trick\n",
    "To make the representation more compact, we can use a hash function to get an index for a column title. \n",
    "it’s one way and once vectorized, the features cannot be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[0.  0.  0.  0.  0.  0.1 0.  0.1 0.2 0.  0.  0.1 0.  0.2 0.  0.1 0.  0.\n",
      "  0.2 0. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy brown dog\"]\n",
    "\n",
    "# Let’s create the HashingVectorizer of a vector size of 10.\n",
    "# transform\n",
    "vectorizer = HashingVectorizer(n_features=20, alternate_sign = False, norm='l1') # default norm is l2, better normalization for document similarity\n",
    "# changed defaults for better understanding\n",
    "# in default settings some will have negative values to compensate for hash collisions:\n",
    "# more: https://github.com/scikit-learn/scikit-learn/issues/7513\n",
    "\n",
    "# create the hashing vector\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize the vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WARNING: keep things in sparse representation,\n",
    "# I am converting here to array for learning\n",
    "type(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "[[0.4 0.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size assumed less than 10 above, if more there will be conflicts!\n",
    "# to demonstrate, lets use vocabulary 2\n",
    "vectorizer = HashingVectorizer(n_features=3, alternate_sign = False, norm='l1') \n",
    "# create the hashing vector\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize the vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "**Term frequency (TF):** Term frequency is simply the ratio of the count of a word present in a document, to the length of the document.   \n",
    "   \n",
    "**Inverse Document Frequency (IDF):** IDF of each word is the log of the ratio of the total number of documents in the document set to the number of documents in the document set that contain that word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "mini_corpus = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "#Tokenize and build vocab\n",
    "vectorizer.fit(mini_corpus)\n",
    "#Summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above example, \"the\" occurs in all documents so has the smallest idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More: \n",
    "Customizing the components of vectorizers:   \n",
    "https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
